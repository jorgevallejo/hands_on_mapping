---
  title: "Las otras ómicas - Mapping"
subtitle: "Análisis Datos Ómicos"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
  toc: true
pdf_document:
  number_sections: true
toc: true
header-includes:
  - \renewcommand{\contentsname}{Índice}

# Next code for knitting both types of documents automatically comes from https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
                    output_format = "all",
                    output_dir = "results") })
# And:
# https://stackoverflow.com/a/46007686/10647267

# bibliography: references.bib
---
  
  ```{r setup, include=FALSE}
# knitr options

# Do not display code in output document
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```

```{r estructura de directorios, results='hide'}
# 'data' contains raw source data.
# 'intermediateData' contains .RData objects with processed data.
# 'results' stores the final report files.

directories <- c("data", "results", "intermediateData", "images")

# Create directories
lapply(directories, function(x){
  if (!(dir.exists(x))){
    dir.create(x)
  }
})

# Remove variable directories
rm(directories)
```

```{r delete results files, eval= FALSE}
# Run this chunk ONLY if you want to re-do
# the complete the report FROM THE ORIGINAL DATA.
# Remember that the .RData files are there to
# avoid unnecesarily redoing of long data processing.

directories <- c("results/", "intermediateData/", "images/")

file.remove(
  # Create a character vector of relative paths
  # to all files in the variable directories
  list.files(path = directories,
             all.files = TRUE,
             full.names = TRUE,
             recursive = TRUE)
)

rm(directories)
```

```{r libraries, include=FALSE}
# Load packages
library(knitr)
library(fastqcr)
```

# Adquisición de datos

Descargamos ficheros FASTQ de ejemplo.
```{r download data, eval=FALSE}
# Training material
# https://zenodo.org/record/61771

urls <- c("https://zenodo.org/record/1324070/files/wt_H3K4me3_read1.fastq.gz",
          "https://zenodo.org/record/1324070/files/wt_H3K4me3_read2.fastq.gz")

for (url in urls){
download.file(url = url,
              destfile = file.path("data", basename(url)),
              method = 'auto')
}

rm(urls)
```

En este caso hemos descargado dos ficheros FastQ correspondientes a datos paired-end, como los que podríamos obtener de un servicio de secuenciación.

# Control de calidad con FastQC

```{r running fastqc}
fastqc(fq.dir = "data/", # Where the FastQ files are
       qc.dir = "results/QC") # Where to store the results

# Generate a table with the results of the reports
kable(qc_aggregate("results/QC"))
```

Los informes en formato HTML se pueden consultar también siguiendo este enlace:
[Carpeta de resultados](./QC/)

La **calidad de secuencias por base** es un poco baja al principio y final de las secuencias, pero no preocupante.

Nos han aparecido avisos en **read2** en **calidad de secuencia por tile** y en **secuencias sobrerrepresentadas**. Tales secuencias sobrerrepresentadas son secuencias compuestas por N; esto es, bases no reconocidas.




# Trimming and filtering
La calidad de las secuencias disminuye  en el extremo de las secuencias. Para evitar sesgos en los análisis, recortaremos esos finales de secuencia de baja calidad con la herramienta [Cutadapt](https://cutadapt.readthedocs.io/en/stable/).

```{r install the tool, eval=FALSE}
system("python3 -m pip install --user --upgrade cutadapt")
```

```{r Use cutadapt}
# Remove reads shorter than 20bp

# Remove ends with low quality (< 20)

# system("cutadapt --quality-cutoff 20,20 --minimum-length 20 -o 'intermediateData/trimmed.fastq'
#        'data/GSM461178_untreat_paired_subset_1.fastq'")

system2('cutadapt',
        args = "--quality-cutoff 20,20 --minimum-length 20 -o 'intermediateData/trimmed.fastq' 'data/GSM461178_untreat_paired_subset_1.fastq'")
```

## Segundo control de calidad

Volvemos a hacer un control de calidad, pero esta vez con los datos pre-procesados.

```{r QC after trimming and filtering}
fastqc(fq.dir = "intermediateData/", # Where the FastQ files are
       qc.dir = "results/QC2") # Where to store the results

# Generate a table with the results of the reports
kable(qc_aggregate("results/QC2"))
```

Los informes en formato HTML se pueden consultar también siguiendo este enlace:
[Carpeta de resultados](./QC2/)


El resultado esperado es que la calidad por base de la secuencia aumente un poco (ya que hemos eliminado extremos de secuencia con baja calidad), y que aparezca un aviso referente a la distribución de longitudes de las secuencias (ya que hemos recortado la longitud de algunas secuencias).